<reference anchor="I-D.wang-cats-innetwork-infer" target="https://datatracker.ietf.org/doc/html/draft-wang-cats-innetwork-infer-00">
  <front>
    <title>In-Network Intelligence for Distributed Collaborative Inference Acceleration</title>
    <author fullname="Hanling Wang" initials="H." surname="Wang">
      <organization>Pengcheng Laboratory</organization>
    </author>
    <author fullname="Qing Li" initials="Q." surname="Li">
      <organization>Pengcheng Laboratory</organization>
    </author>
    <author fullname="Yong Jiang" initials="Y." surname="Jiang">
      <organization>Tsinghua Shenzhen International Graduate School, Pengcheng Laboratory</organization>
    </author>
    <date year="2025" month="September" day="15"/>
    <abstract>
      <t>The rapid proliferation of deep learning models has led to growing demands for low-latency and high-throughput inference across heterogeneous environments. While edge devices often host data sources, their limited compute and network resources restrict efficient model inference. Cloud servers provide abundant capacity but suffer from transmission delays and bottlenecks. Emerging programmable in-network devices (e.g., switches, FPGAs, SmartNICs) offer a unique opportunity to accelerate inference by processing tasks directly along data paths. This document introduces an architecture for _Distributed Collaborative Inference Acceleration_. It proposes mechanisms to split, offload, and coordinate inference workloads across edge devices, in-network resources, and cloud servers, enabling reduced response time and improved utilization.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-wang-cats-innetwork-infer-00"/>
</reference>