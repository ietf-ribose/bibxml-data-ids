<reference anchor="I-D.zhang-rtgwg-llmmoe-multicast" target="https://datatracker.ietf.org/doc/html/draft-zhang-rtgwg-llmmoe-multicast-01">
  <front>
    <title>Multicast usage in LLM MoE</title>
    <author fullname="Zheng Zhang" initials="Z." surname="Zhang">
      <organization>ZTE Corporation</organization>
    </author>
    <author fullname="Wei Duan" initials="W." surname="Duan">
      <organization>ZTE Corporation</organization>
    </author>
    <author fullname="Xiaohu Xu" initials="X." surname="Xu">
      <organization>China Mobile</organization>
    </author>
    <date year="2025" month="October" day="20"/>
    <abstract>
      <t>Large Language Models (LLMs) have been widely used in recent years. The Mixture of Experts (MoE) architecture is one of the features of LLMs that enables efficient inference and cost-effective training. With the MoE architecture, there are potential multicast use cases such as tokens dispatching. This draft attempts to analyze these use cases.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-zhang-rtgwg-llmmoe-multicast-01"/>
</reference>