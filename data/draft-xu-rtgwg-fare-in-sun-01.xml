<reference anchor="I-D.xu-rtgwg-fare-in-sun" target="https://datatracker.ietf.org/doc/html/draft-xu-rtgwg-fare-in-sun-01">
  <front>
    <title>Fully Adaptive Routing Ethernet in Scale-Up Networks</title>
    <author fullname="Xiaohu Xu" initials="X." surname="Xu">
      <organization>China Mobile</organization>
    </author>
    <author fullname="Zongying He" initials="Z." surname="He">
      <organization>Broadcom</organization>
    </author>
    <author fullname="Hua Wang" initials="H." surname="Wang">
      <organization>Moore Threads</organization>
    </author>
    <author fullname="Tianyou Zhou" initials="T." surname="Zhou">
      <organization>Resnics Technology</organization>
    </author>
    <author fullname="Yongtao Yang" initials="Y." surname="Yang">
      <organization>Centec</organization>
    </author>
    <author fullname="Yinben Xia" initials="Y." surname="Xia">
      <organization>Tencent</organization>
    </author>
    <author fullname="Peilong Wang" initials="P." surname="Wang">
      <organization>Baidu</organization>
    </author>
    <author fullname="Yan Zhuang" initials="Y." surname="Zhuang">
      <organization>Huawei Technologies</organization>
    </author>
    <author fullname="Fajie Yang" initials="F." surname="Yang">
      <organization>Cloudnine Information Technologies</organization>
    </author>
    <author fullname="Chao Li" initials="C." surname="Li">
      <organization>Metanet Networking Technology</organization>
    </author>
    <author fullname="Xiaojun Wang" initials="X." surname="Wang">
      <organization>Ruijie Networks</organization>
    </author>
    <date year="2025" month="May" day="21"/>
    <abstract>
      <t>The Mixture of Experts (MoE) has become a dominant paradigm in transformer-based artificial intelligence (AI) large language models (LLMs). It is widely adopted in both distributed training and distributed inference. Furthermore, the disaggregation of the prefill and decode phases is highly beneficial and is considered a best practice for distributed inference models; however, this approach depends on highly efficient Key-Value (KV) cache synchronization. To enable efficient expert parallelization and KV cache synchronization across dozens or even hundreds of Graphics Processing Units (GPUs) in MoE architectures, an ultra-high- throughput, ultra-low-latency AI scale-up network (SUN) that can efficiently distribute data across all network planes is critical. This document describes how to extend the Weighted Equal-Cost Multi- Path (WECMP) load-balancing mechanism, referred to as Fully Adaptive Routing Ethernet (FARE), which was originally designed for scale-out networks, to scale-up networks.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-xu-rtgwg-fare-in-sun-01"/>
</reference>