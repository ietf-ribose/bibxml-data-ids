<reference anchor="I-D.hong-nmrg-ai-deploy">
  <front>
    <title>Considerations of deploying AI services in a distributed approach</title>
    <author fullname="Yong-Geun Hong" initials="Y." surname="Hong">
      <organization>Daejeon University</organization>
    </author>
    <author fullname="Oh Seokbeom" initials="O." surname="Seokbeom">
      <organization>KSA</organization>
    </author>
    <author fullname="SooJeong Lee" initials="S." surname="Lee">
      <organization>Korea University/KT</organization>
    </author>
    <author fullname="Hyun-Kook Kahng" initials="H." surname="Kahng">
      <organization>Korea University</organization>
    </author>
    <date year="2022" month="March" day="7"/>
    <abstract>
      <t>As the development of AI technology matured and AI technology began to be applied in various fields, AI technology is changed from running only on very high-performance servers with small hardware, including microcontrollers, low-performance CPUs and AI chipsets. In this document, we consider how to configure the system in terms of AI inference service to provide AI service in a distributed approach. Also, we describe the points to be considered in the environment where a client connects to a cloud server and an edge device and requests an AI service.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-hong-nmrg-ai-deploy-00"/>
  <format type="TXT" target="https://www.ietf.org/archive/id/draft-hong-nmrg-ai-deploy-00.txt"/>
</reference>