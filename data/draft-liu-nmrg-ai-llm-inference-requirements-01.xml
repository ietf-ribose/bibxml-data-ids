<reference anchor="I-D.liu-nmrg-ai-llm-inference-requirements" target="https://datatracker.ietf.org/doc/html/draft-liu-nmrg-ai-llm-inference-requirements-01">
  <front>
    <title>Requirements Analysis of System and Network for Large Language Model Inference Service</title>
    <author fullname="Liu Chang" initials="L." surname="Chang">
      <organization>China Mobile</organization>
    </author>
    <author fullname="Chuyi Guo" initials="C." surname="Guo">
      <organization>China Mobile</organization>
    </author>
    <date year="2025" month="July" day="7"/>
    <abstract>
      <t>With the rise of ChatGPT, DeepSeek, and other Large Language Models, which is short for LLMs in the remaining part, as well as the proliferation of inference applications, inference serving oriented to large-scale users has become increasingly critical. However, due to the extreme demands on computing power and communication during inference, the large-scale service deployment of LLMs poses significant challenges. To address these challenges, different vendors have adopted diverse inference service architectures, such as vLLM, SGLang, Mooncake, etc. This paper investigates mainstream inference frameworks, summarizes their core design principle and research question, and analyzes the challenges and requirements they impose on network management. The goal is to lay a foundation for defining a unified LLM inference architecture in the future.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-liu-nmrg-ai-llm-inference-requirements-01"/>
</reference>