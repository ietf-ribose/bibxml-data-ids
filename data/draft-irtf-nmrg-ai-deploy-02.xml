<reference anchor="I-D.irtf-nmrg-ai-deploy" target="https://datatracker.ietf.org/doc/html/draft-irtf-nmrg-ai-deploy-02">
  <front>
    <title>Considerations of network/system for AI services</title>
    <author fullname="Yong-Geun Hong" initials="Y." surname="Hong">
      <organization>Daejeon University</organization>
    </author>
    <author fullname="Joo-Sang Youn" initials="J." surname="Youn">
      <organization>DONG-EUI University</organization>
    </author>
    <author fullname="Seung-Woo Hong" initials="S." surname="Hong">
      <organization>ETRI</organization>
    </author>
    <author fullname="Pedro Martinez-Julia" initials="P." surname="Martinez-Julia">
      <organization>National Institute of Information and Communications Technology</organization>
    </author>
    <author fullname="Qin Wu" initials="Q." surname="Wu">
      <organization>Huawei</organization>
    </author>
    <date year="2025" month="October" day="20"/>
    <abstract>
      <t>As the development of AI technology has matured and AI technology has begun to be applied in various fields, it has changed from running only on very high-performance servers to running on commodity servers, with affordable, small-scale hardware, including microcontrollers, low-performance CPUs, and AI chipsets. This document outlines how to configure the network and system for an AI inference service, providing AI services in a distributed manner. It also outlines the factors to consider when a client connects to a cloud server and an edge device to requests an AI service. It describes some use cases for deploying network-based AI services, such as self-driving vehicles and network digital twins.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-irtf-nmrg-ai-deploy-02"/>
</reference>