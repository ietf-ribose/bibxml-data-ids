<reference anchor="I-D.kompella-rtgwg-mlnwsched" target="https://datatracker.ietf.org/doc/html/draft-kompella-rtgwg-mlnwsched-01">
  <front>
    <title>Scheduling Network Resources for Machine Learning Clusters</title>
    <author fullname="Kireeti Kompella" initials="K." surname="Kompella">
      <organization>Juniper Networks</organization>
    </author>
    <author fullname="Vishnu Pavan Beeram" initials="V. P." surname="Beeram">
      <organization>Juniper Networks</organization>
    </author>
    <author fullname="Aditya Mahale" initials="A." surname="Mahale">
      <organization>Cerebras Systems</organization>
    </author>
    <author fullname="Raghav Bhargava" initials="R." surname="Bhargava">
      <organization>Crusoe</organization>
    </author>
    <date year="2025" month="November" day="2"/>
    <abstract>
      <t>Large Language Models (LLMs) are pushing the boundaries of technology. The scale that they have reached currently vastly exceeds the capacity of any single compute unit (XPU); this requires a distributed approach where multiple XPUs are connected via a "backend" network, sometimes in a single data center, but increasingly in multiple data centers connected by a "data center interconnect" (DCI). We are approaching the point where the scale exceeds that of a single data center, thus requiring multiple such data centers connected via a "data center interconnect" network. Training and inferencing are expensive and critical operations, thus they are typically scheduled, i.e., the (compute) resources they need are carefully estimated, allocated and deployed so that these resources are efficiently used. However, while compute investment in these LLM processing clusters dwarfs that of networks, it is becoming increasingly clear that the latter can greatly impact the former. This has been the focus of recent conferences, including the fantel Birds of a Feather meeting in IETF 123, @Scale: Networking 2025 and Open Compute Project 2025. This memo proposes that the same care that is taken regarding allocation of compute resources to jobs be taken with networking resources: that they are estimated, allocated and deployed alongside compute resources; that they have contingency plans in case of network glitches; and that a holistic view be taken in order to optimize job completion times of training and inferencing jobs.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-kompella-rtgwg-mlnwsched-01"/>
</reference>