<reference anchor="I-D.ai-traffic" target="https://datatracker.ietf.org/doc/html/draft-ai-traffic-00">
  <front>
    <title>Handling inter-DC/Edge AI-related network traffic: Problem statement</title>
    <author fullname="Antoine Fressancourt" initials="A." surname="Fressancourt">
      <organization>Huawei Technologies France S.A.S.U.</organization>
    </author>
    <author fullname="Luigi Iannone" initials="L." surname="Iannone">
      <organization>Huawei Technologies France S.A.S.U.</organization>
    </author>
    <author fullname="Zhe Lou" initials="Z." surname="Lou">
      <organization>Huawei Technologies Duesseldorf GmbH</organization>
    </author>
    <author fullname="Dirk Trossen" initials="D." surname="Trossen">
      <organization>Huawei Technologies Duesseldorf GmbH</organization>
    </author>
    <date year="2024" month="October" day="7"/>
    <abstract>
      <t>The growth in terms of number of parameters of LLM models as well as the need to use or train those models with private or protected data will require service providers operating LLM-based services to cooperate to train, specialize or serve LLM-based services accross datacenters. Given their structure, the number of parameters they incorporate and the collective communication librairies they are built with, LLM training and inference (or serving) network traffic has specific characteristics. In that regard, understanding the specificities of AI-related workloads is critical to determine how to operate AI-based services in a federated setting across datacenters.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-ai-traffic-00"/>
</reference>