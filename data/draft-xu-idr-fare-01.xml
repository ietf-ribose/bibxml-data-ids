<reference anchor="I-D.xu-idr-fare" target="https://datatracker.ietf.org/doc/html/draft-xu-idr-fare-01">
  <front>
    <title>Fully Adaptive Routing Ethernet using BGP</title>
    <author fullname="Xiaohu Xu" initials="X." surname="Xu">
      <organization>China Mobile</organization>
    </author>
    <author fullname="Shraddha Hegde" initials="S." surname="Hegde">
      <organization>Juniper</organization>
    </author>
    <author fullname="Zongying He" initials="Z." surname="He">
      <organization>Broadcom</organization>
    </author>
    <author fullname="Junjie Wang" initials="J." surname="Wang">
      <organization>Centec</organization>
    </author>
    <author fullname="Hongyi Huang" initials="H." surname="Huang">
      <organization>Huawei</organization>
    </author>
    <author fullname="Qingliang Zhang" initials="Q." surname="Zhang">
      <organization>H3C</organization>
    </author>
    <author fullname="Hang Wu" initials="H." surname="Wu">
      <organization>Ruijie Networks</organization>
    </author>
    <author fullname="Yadong Liu" initials="Y." surname="Liu">
      <organization>Tencent</organization>
    </author>
    <author fullname="Yinben Xia" initials="Y." surname="Xia">
      <organization>Tencent</organization>
    </author>
    <author fullname="Peilong Wang" initials="P." surname="Wang">
      <organization>Baidu</organization>
    </author>
    <author fullname="Tiezheng" initials="" surname="Tiezheng">
      <organization>IEIT SYSTEMS</organization>
    </author>
    <date year="2024" month="July" day="23"/>
    <abstract>
      <t>Large language models (LLMs) like ChatGPT have become increasingly popular in recent years due to their impressive performance in various natural language processing tasks. These models are built by training deep neural networks on massive amounts of text data, often consisting of billions or even trillions of parameters. However, the training process for these models can be extremely resource- intensive, requiring the deployment of thousands or even tens of thousands of GPUs in a single AI training cluster. Therefore, three- stage or even five-stage CLOS networks are commonly adopted for AI networks. The non-blocking nature of the network become increasingly critical for large-scale AI models. Therefore, adaptive routing is necessary to dynamically load balance traffic to the same destination over multiple ECMP paths, based on network capacity and even congestion information along those paths.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-xu-idr-fare-01"/>
</reference>