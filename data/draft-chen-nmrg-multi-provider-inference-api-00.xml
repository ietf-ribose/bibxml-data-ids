<reference anchor="I-D.chen-nmrg-multi-provider-inference-api" target="https://datatracker.ietf.org/doc/html/draft-chen-nmrg-multi-provider-inference-api-00">
  <front>
    <title>Multi-Provider Extensions for Agentic AI Inference APIs</title>
    <author fullname="Huamin Chen" initials="H." surname="Chen">
      <organization>Red Hat</organization>
    </author>
    <author fullname="Luay Jalil" initials="L." surname="Jalil">
      <organization>Verizon</organization>
    </author>
    <author fullname="Nabeel Cocker" initials="N." surname="Cocker">
      <organization>Red Hat</organization>
    </author>
    <date year="2025" month="October" day="19"/>
    <abstract>
      <t>This document specifies extensions for multi-provider distributed AI inference using the widely-adopted OpenAI Responses API as the reference interface standard. These extensions enable provider diversity, load balancing, failover, and capability negotiation in distributed inference environments while maintaining full backward compatibility with existing implementations. The extensions do not require changes to standard API usage patterns or existing client applications. By treating the OpenAI Responses API as a de facto standard interface (similar to how HTTP serves as a standard protocol), these extensions provide an optional enhancement layer for multi-provider orchestration, intelligent routing, and distributed inference capabilities. The approach preserves the familiar API interface that developers already know and use, while enabling seamless integration across multiple AI inference providers without vendor lock-in.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-chen-nmrg-multi-provider-inference-api-00"/>
</reference>