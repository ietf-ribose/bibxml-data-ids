<reference anchor="I-D.xu-rtgwg-fare-in-sun" target="https://datatracker.ietf.org/doc/html/draft-xu-rtgwg-fare-in-sun-00">
  <front>
    <title>Fully Adaptive Routing Ethernet in Scale-Up Networks</title>
    <author fullname="Xiaohu Xu" initials="X." surname="Xu">
      <organization>China Mobile</organization>
    </author>
    <date year="2025" month="May" day="10"/>
    <abstract>
      <t>The Mixture of Experts (MoE) has become a dominant paradigm in transformer-based artificial intelligence (AI) large language models (LLMs). It is widely adopted in both distributed training and distributed inference. Furthermore, the disaggregation of the prefill and decode phases is highly beneficial and is considered a best practice for distributed inference models; however, this approach depends on highly efficient Key-Value (KV) cache synchronization. To enable efficient expert parallelization and KV cache synchronization across dozens or even hundreds of Graphics Processing Units (GPUs) in MoE architectures, an ultra-high- throughput, ultra-low-latency AI scale-up network (SUN) that can efficiently distribute data across all network planes is critical. This document describes how to extend the Weighted Equal-Cost Multi- Path (WECMP) load-balancing mechanism, referred to as Fully Adaptive Routing Ethernet (FARE), which was originally designed for scale-out networks, to scale-up networks.</t>
    </abstract>
  </front>
  <seriesInfo name="Internet-Draft" value="draft-xu-rtgwg-fare-in-sun-00"/>
</reference>